{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing process:\n",
    "1. Make vocab and choose top k words as the features\n",
    "   :-  there is a makeDictionary function for this\n",
    "   :- while making vocab stop words are removed\n",
    "   :- lammatozer is used to have meaningfull words\n",
    "   :- now dictionary is sorted in reverse order\n",
    "   :- vocab is the list of top k words from the dictionary\n",
    "   :- I have used k = 12000\n",
    "2. Fit the data\n",
    "   :- there is a fit function for this\n",
    "   :- fit function takes two parameters path( where all 20 news groups are present ) and vocab\n",
    "   :- fit function returns train_data in the form of nested dictionary as\n",
    "   \n",
    "                                                                            |---[word_1]---count\n",
    "                                                                            |---[word_2]---count\n",
    "                                                                            |\n",
    "                                                                            |\n",
    "                                                                            |---[word_n]---count\n",
    "                                                                            |\n",
    "                                                        |---[folder1_name]--|---[documents]---count\n",
    "                                                        |                   |---[total_words]---count\n",
    "                                                        |\n",
    "                                          train_data----|---[folder2_name]--|--\n",
    "                                                        |---[folder3_name]--|--\n",
    "                                                        |\n",
    "                                                        |\n",
    "                                                        |---[folder20_name]--|--\n",
    "                                                        |\n",
    "                                                        |---[total_documents]---count\n",
    "3. Predict the data\n",
    "   :- there is a predict function for this\n",
    "   :- predict function takes two parameters train_data and test_data\n",
    "   :- testData function returns the test_data..test_data have the words for a perticular file in a folder\n",
    "   :- predict function predicts the best class\n",
    "   :- probability function returns the probability of a given file for a perticular class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import operator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Abhishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Abhishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "stop_words|={\"\", \",\" , \".\", \"-\",\" \",\"also\", \"even\",\"to\",\"it\",\"hi\",\"ha\", \"know\",\"use\",\"however\", \"xref\", \"cantaloupesrvcscmuedu\"\n",
    "            ,\"wa\",\"gmt\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"june\",\"july\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\",\"mon\",\"tue\",\"wed\",\"thu\",\"fri\",\n",
    "            \"sat\",\"sun\",\"nntp\",\"doe\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = str.maketrans('', '', string.punctuation)\n",
    "wnl=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeDictionary(path):\n",
    "    word_count = {}\n",
    "    for folder in os.listdir(path):\n",
    "        path_for_files_in_folder=path+\"\\\\\"+folder\n",
    "        for file in os.listdir(path_for_files_in_folder):\n",
    "            absolute_file_path = path_for_files_in_folder + \"\\\\\" + file\n",
    "            doc_file = open( absolute_file_path , \"r\" , errors=\"ignore\" )\n",
    "            doc_text = re.split(' |,|-|\\n' , doc_file.read().strip() )\n",
    "            for word in doc_text:\n",
    "                if word.isalpha():\n",
    "                    my_word = wnl.lemmatize(word.strip(\" \").translate(table).lower())\n",
    "                    word_length = len(my_word)\n",
    "                    if my_word in stop_words or word_length < 2 or word_length > 15:\n",
    "                        continue\n",
    "                    elif my_word in word_count:\n",
    "                        word_count[my_word] += 1\n",
    "                    else:\n",
    "                        word_count[my_word] = 1\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Abhishek\\Desktop\\myDataSets\\text_classification\\20_newsgroups\"\n",
    "dictionary=makeDictionary(path)\n",
    "sorted_dictionary=sorted(dictionary.items(), key=operator.itemgetter(1),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab=list()\n",
    "for i in range(12000):\n",
    "    vocab.append(sorted_dictionary[i][0]) # vocab contains top 12001 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit( path , vocab ):\n",
    "    train_data = {}\n",
    "    total_doc_count = 0  #count of total documents in all the folder\n",
    "    for folder in os.listdir(path):\n",
    "        train_data[folder] = {}\n",
    "        doc_count = 0  #count of documents in a folder\n",
    "        total_words = 0  #count of words in a folder \n",
    "        path_for_files_in_folder = path + \"\\\\\" + folder\n",
    "        for file in os.listdir(path_for_files_in_folder):\n",
    "            doc_count += 1\n",
    "            absolute_file_path = path_for_files_in_folder + \"\\\\\" + file\n",
    "            doc_file = open( absolute_file_path, \"r\" , errors = \"ignore\" )\n",
    "            doc_text = re.split(' |,|-|\\n' , doc_file.read().strip() ) # splits the document about these characters\n",
    "            for word in doc_text:\n",
    "                if word.isalpha():  #check if word is alphabet or not\n",
    "                    my_word = wnl.lemmatize(word.strip(\" \").translate(table).lower())\n",
    "                    if my_word in vocab: #check if the word is present in vocab or not\n",
    "                        total_words += 1\n",
    "                        if my_word in train_data[folder]:\n",
    "                            train_data[folder][my_word] += 1\n",
    "                        else:\n",
    "                            train_data[folder][my_word] = 1\n",
    "        total_doc_count += doc_count\n",
    "        train_data[folder][\"doc_count\"] = doc_count\n",
    "        train_data[folder][\"total_words\"] = total_words\n",
    "    train_data[\"total_doc_count\"] = total_doc_count\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def probability(train_data , current_class , test_data):\n",
    "    class_prob = np.log(train_data[current_class][\"doc_count\"])-np.log(train_data[\"total_doc_count\"])\n",
    "    for word in test_data:\n",
    "        if word == \"total_words\":\n",
    "            continue\n",
    "        else:\n",
    "            if word in train_data[current_class]:\n",
    "                word_count_in_current_class = train_data[current_class][word] + 1 # 1 is added as laplase correction\n",
    "            else:\n",
    "                word_count_in_current_class = 1\n",
    "            total_words_in_current_class = train_data[current_class][\"total_words\"] + test_data[\"total_words\"] # laplase correction\n",
    "            word_prob = np.log(word_count_in_current_class) - np.log(total_words_in_current_class)\n",
    "            class_prob = class_prob + word_prob\n",
    "    return class_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(train_data,test_data):\n",
    "    best_prob = -1000\n",
    "    best_class = \"\"\n",
    "    first_run = True\n",
    "    classes = train_data.keys()\n",
    "    for current_class in classes:\n",
    "        if current_class == \"total_doc_count\":\n",
    "            continue\n",
    "        prob = probability(train_data , current_class , test_data)\n",
    "        #print(\"prob \",prob,\"current_class \",current_class)\n",
    "        if first_run or (prob > best_prob):\n",
    "            best_prob = prob\n",
    "            best_class = current_class\n",
    "        first_run = False\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testData( path , vocab ):\n",
    "    test_data={}\n",
    "    test_data[\"total_words\"]=0\n",
    "    #for file in os.listdir(path):\n",
    "        #file_path = path + \"\\\\\" + file\n",
    "    doc_file = open( path, \"r\" , errors = \"ignore\" )\n",
    "    doc_text = re.split(' |,|-|\\n' , doc_file.read().strip() )\n",
    "    for word in doc_text:\n",
    "        if word.isalpha():\n",
    "            my_word = wnl.lemmatize(word.strip(\" \").translate(table).lower())\n",
    "            if my_word in vocab:\n",
    "                if my_word in test_data:\n",
    "                    continue\n",
    "                else:\n",
    "                    test_data[my_word] = 1\n",
    "                    test_data[\"total_words\"] += 1\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_path = r\"C:\\Users\\Abhishek\\Desktop\\myDataSets\\text_classification\\20_newsgroups\"\n",
    "test_data_path = r\"C:\\Users\\Abhishek\\Desktop\\myDataSets\\text_classification\\mini_newsgroups\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = fit(train_data_path , vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count=1\n",
    "Y_pred = list()\n",
    "for folder in os.listdir(test_data_path):\n",
    "    file_path = test_data_path + \"\\\\\" + folder\n",
    "    for file in os.listdir(file_path):\n",
    "        file_text_path = file_path + \"\\\\\" + file \n",
    "        test_data = testData( file_text_path , vocab)\n",
    "        predicted_class = predict(train_data , test_data)\n",
    "        #print(predicted_class)\n",
    "        if predicted_class == folder:\n",
    "            Y_pred.append(int(1))\n",
    "        else:\n",
    "            Y_pred.append(int(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_socre =  0.8855\n"
     ]
    }
   ],
   "source": [
    "one = list()\n",
    "for i in range(len(Y_pred)):\n",
    "    one.append(int(1))\n",
    "Y_pred = np.array(Y_pred)\n",
    "one = np.array(one)\n",
    "print(\"my_socre = \",np.mean(Y_pred == one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to to make the dictionary so as to convert this dictionary into dataframe..\n",
    "def makeData(path , features):\n",
    "    train_data = {}\n",
    "    for folder in os.listdir(path):\n",
    "        s=time()\n",
    "        file_path = path + \"\\\\\" + folder\n",
    "        for file in os.listdir(file_path):\n",
    "            train_data[file] = {}\n",
    "            for w in features:\n",
    "                train_data[file][w] = 0\n",
    "            text_path = file_path + \"\\\\\" + file\n",
    "            doc_text = open( text_path, \"r\" , errors = \"ignore\" )\n",
    "            doc_words = re.split(' |,|-|\\n' , doc_text.read().strip() )\n",
    "            for word in doc_words:\n",
    "                if word.isalpha():\n",
    "                    my_word = wnl.lemmatize(word.strip(\" \").translate(table).lower())\n",
    "                    if my_word in stop_words:\n",
    "                        continue\n",
    "                    if my_word in features:\n",
    "                        if my_word in train_data[file]:\n",
    "                            train_data[file][my_word] += 1\n",
    "                        else:\n",
    "                            train_data[file][my_word] = 1\n",
    "            train_data[file][\"zz_class\"] = folder\n",
    "        e = time()\n",
    "        print(e-s)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features={}\n",
    "for i in range(12000):\n",
    "    features[sorted_dictionary[i][0]] = 1 # features contains top 2000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_path = r\"C:\\Users\\Abhishek\\Desktop\\myDataSets\\text_classification\\20_newsgroups\"\n",
    "test_data_path2 = r\"C:\\Users\\Abhishek\\Desktop\\myDataSets\\text_classification\\mini_newsgroups\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data2 = makeData(train_data_path , features)\n",
    "train_df = pd.DataFrame(train_data2).T.fillna(0)\n",
    "#test_data2 = makeData(test_data_path2 , features)\n",
    "#test_df = pd.DataFrame(test_data2).T.fillna(0)\n",
    "#train_df.shape\n",
    "#test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(train_df.iloc[:,0:12000], train_df.iloc[:,12000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "socre =  0.772773797339\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(test_df.iloc[:,0:12000])\n",
    "print(\"socre = \" , np.mean(y_pred == test_df.iloc[: , 12000]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
